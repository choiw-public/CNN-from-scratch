{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mnist # pip install mnist\n",
    "import utils\n",
    "import operations as op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:\n",
    "# increase total_iteration to get a higher accuracy\n",
    "np.random.seed(0)\n",
    "batch_size = 100\n",
    "total_iteration = 1000\n",
    "\n",
    "lr = 0.1 # learning rate\n",
    "weight_decay = 0.0\n",
    "optimizer = 'gradient_descent' # optional: momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:\n",
    "# os.FC : fully connected layer\n",
    "\n",
    "# Usage:\n",
    "# os.FC(node_in, node_out, use_bias)\n",
    "\n",
    "\n",
    "model_np = [op.FC(784, 32, False),\n",
    "            op.BatchNormalization(),\n",
    "            op.ReLU(),\n",
    "            op.FC(32, 10, False),\n",
    "            op.Softmax()]\n",
    "softmax_np = op.Softmax()\n",
    "loss_np = op.CrossEntropy() # loss\n",
    "cost_np = op.Cost() # average of losses over batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0100 | cost:1.889,  accuracy:0.69\n",
      "iter:0200 | cost:1.753,  accuracy:0.83\n",
      "iter:0300 | cost:1.709,  accuracy:0.85\n",
      "iter:0400 | cost:1.686,  accuracy:0.88\n",
      "iter:0500 | cost:1.647,  accuracy:0.86\n",
      "iter:0600 | cost:1.617,  accuracy:0.90\n",
      "iter:0700 | cost:1.634,  accuracy:0.87\n",
      "iter:0800 | cost:1.582,  accuracy:0.92\n",
      "iter:0900 | cost:1.600,  accuracy:0.88\n",
      "iter:1000 | cost:1.605,  accuracy:0.87\n"
     ]
    }
   ],
   "source": [
    "# prepare train data\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "\n",
    "args = {'lr': lr, 'is_train': True, 'optimizer': optimizer, 'weight_decay': weight_decay}\n",
    "for iteration in range(total_iteration):\n",
    "    random_indices = np.random.randint(0, len(train_images), batch_size)\n",
    "    x_np = train_images[random_indices, ::].reshape(batch_size, 784) / 255.0\n",
    "    y_np = utils.one_hot_encode(train_labels[random_indices], 10)\n",
    "    args['y'] = y_np\n",
    "\n",
    "    # forward\n",
    "    out_np = x_np\n",
    "    l2_loss = 0\n",
    "    for layer in model_np:\n",
    "        layer.forward(out_np, args)\n",
    "        try:\n",
    "            l2_loss += layer.l2_los\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        out_np = layer.value\n",
    "    softmax_np.forward(out_np, args)\n",
    "    loss_np.forward(softmax_np.value, args)\n",
    "    cost_np.forward(loss_np.value, l2_loss)\n",
    "    pred_np = np.argmax(model_np[-1].value, 1)\n",
    "    accuracy_np = np.mean(pred_np == np.argmax(y_np, 1))\n",
    "    \n",
    "    # backward\n",
    "    cost_np.backward()\n",
    "    loss_np.backward(cost_np.grads)\n",
    "    softmax_np.backward(loss_np.grads)\n",
    "    grads_np = softmax_np.grads\n",
    "    for layer in model_np[::-1]:\n",
    "        layer.backward(grads_np, args)\n",
    "        grads_np = layer.grads\n",
    "    if (iteration+1) % 100 == 0:\n",
    "        print(\"iter:%04d | cost:%.3f,  accuracy:%.2f\" % (iteration+1, cost_np.value, accuracy_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing (01000/10000)\n",
      "Testing (02000/10000)\n",
      "Testing (03000/10000)\n",
      "Testing (04000/10000)\n",
      "Testing (05000/10000)\n",
      "Testing (06000/10000)\n",
      "Testing (07000/10000)\n",
      "Testing (08000/10000)\n",
      "Testing (09000/10000)\n",
      "Testing (10000/10000)\n",
      "accuracy: 0.920\n"
     ]
    }
   ],
   "source": [
    "# prepare test data\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "correctness = 0.0\n",
    "args['is_train'] = False # to use moving mean and variance\n",
    "for i, (test_img, test_label) in enumerate(zip(test_images, test_labels)):\n",
    "    x_np = (test_img.reshape([1, 784]) / 255.0).astype(np.float32)\n",
    "    y_np = utils.one_hot_encode(test_label.reshape(1, 1), 10)\n",
    "    # forward\n",
    "    out_np = x_np\n",
    "    for layer in model_np:\n",
    "        layer.forward(out_np, args)\n",
    "        out_np = layer.value\n",
    "    pred_np = np.argmax(out_np, 1)\n",
    "    correctness += np.sum(pred_np == test_label)\n",
    "    if (i+1) % 1000 == 0:\n",
    "        print(\"Testing (%05d/%05d)\"%(i+1, len(test_images)))\n",
    "print(\"accuracy: %.3f\" % (correctness / len(test_images)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
